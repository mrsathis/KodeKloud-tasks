1) Monitor the logs of pod "foo"                                                 

   extract log lines corresponding to error "file_not found"
   write them to file /opt/..
   
   Ans:
   
   kubectl logs foo > /opt/..
   
   
2) sort PV's by capacity                                                       

   Ans:
   
   kubectl get pv --sort-by=.spec.capacity.storage
   
3) Add an init container to lumpsy-kolal (Spec file is provided)                
   
   init container should create an empty file named /workdir/eager.txt
   
   if /workdir/eager.txt is not found then pod should exit
	
	Once spec file is updated create a new pod
	
	Ans:
	
apiVersion: v1
kind: Pod
metadata:
  name: myappinit
  labels:
    app: myapp
spec:
  volumes:
  - name: cache-volume
    emptyDir: {}
  containers:
  - name: myapp-container
    image: alpine
	
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'mkdir workdir && touch workdir/myfile.txt || exit 1;']
	
##  command: ['sh', '-c', 'mkdir1 /work; echo>/work/sharedfile1.txt', if [[-f "/work/sharedfile1.txt"] 	
	
  command: ['sh', '-c', 'touch workdir/file.txt; state=$?; if [[ $state -ne 0 ]]; then exit 1; fi;']





4) create pod named kucc4 with single app container for each image redis+nginx         --> 

apiVersion: v1
kind: Pod
metadata:
  name: kucc4
spec:
  
  containers:
  - name: 1st
    image: nginx
    
  - name: 2nd
    image: redis	
	
	
	
5) create a pod as follows name: nginx-kubs image=nginx Node-selector: disk=spinning

 
apiVersion: v1
kind: Pod
metadata:
  name: nginx-kubs
  
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disk: spinning
	
	
	
6) create a deployment name: nginx-app ---using nginx:1.11.10-alphine --should have 3 replicas
   next deploy nginx:1.13.0-alphine by rolling update and record then rollback to previous version
   

kubectl create deployment nginx-app --image=nginx:1.11.10-alphine --dry-run -o yaml   
kubectl set image deployment.apps/nginx-app nginx=nginx:1.13.0-alphine --record

kubectl rollout undo deployment.apps/nginx-app


7)  create and configure a service front-end-service so its accessible through Node-port and routes to existing pod front-end-service

kubectl run front-end --image=nginx
kubectl expose front-end --name=front-end-service 

apiVersion: v1
kind: Service
metadata:
  name: front-end-service
spec:
  type: NodePort
  selector:
    app: front-end
  ports:
      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.
    - port: 80
      targetPort: 80
      # Optional field
      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)
      nodePort: 30007
	  
	  

8) create a pod as follows

name: mongo
image : mongo
namespace: website-frontend

Ans:

kubectl run mongo --image=mongo -n website-frontend --dry-run -o yaml


master $ kubectl run front-end-service --image=nginx --dry-run -o yaml
W0703 10:55:01.576419   10175 helpers.go:535] --dry-run is deprecated and can be replaced with --dry-run=client.
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: front-end-service
  name: front-end-service
spec:
  containers:
  - image: nginx
    name: front-end-service
    resources: {}
  dnsPolicy: ClusterFirst

=================================================================

9) Create a deployment spec file that will 

    launch 2 replicas of nginx image with label app_env_stage=dev
	deployment name: kual0021
	
	save copy to /opt/spec.yaml
	
	when done clear any kubernetes object created during this task..


ANS:


apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app_env_stage: dev
  name: kual
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kual
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: kual
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}

===========================================================
  
10) create a file /opt/pod.txt that lists all pods that implement service baz in namespace Kube-system

kubectl -n production get svc svc1 --show-labels
kubectl -n production get pods --selector="app_env_stage=dev" > /opt/KUCC00302/kucc00302.txt


kubectl describe svc my-nginx

Name:                my-nginx
Namespace:           default
Labels:              run=my-nginx
Annotations:         <none>
Selector:            run=my-nginx
Type:                ClusterIP
IP:                  10.0.162.149
Port:                <unset> 80/TCP
Endpoints:           10.244.2.5:80,10.244.3.4:80
Session Affinity:    None
Events:              <none>

================================

11) create a k8s secret as follows

    . Name  --> Super-secret
	. password --> alice
	
	
	create a pod named pod-secret-via-file, using redis image... which mounts a secret named SUPER-SECRET at /secrets
	
	create a second pod named pod-secret-via-env, using the redis image which exports password as CREDENTIALS
	
	
	kubectl create secret generic dev-db-secret --from-literal=username=Super-secret --from-literal=password='alice'
	
	apiVersion: v1
kind: Pod
metadata:
  name: pod-secret-via-env
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: super-secret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: super-secret
            key: password
			
			
			kubectl exec -it podname sh   --> env | grep secrets
	
	
=======================================

12) Create a pod as follows

    . Name: non-persistent-redis
	. Container Image: redis
	. Volume with name: cache-control
	. Mount Path: /data/redis
	
The pod should launch in staging namespace and volume should not be persistemnt
=====================================

13) Scale a deployment webserver to 6 pods

kubectl scale --current-replicas=2 --replicas=6 deployment/webserver

==========================================

14) Check to see how many nodes are ready, not including the nodes which are tainted:no schedule and write the no to file.

kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINT:.spec.taints[*].effect | grep -v NoSchedule
    kubectl describe nodes
-----------------------------------------------

15) From the pod label name=cpu-loader, find pods with high CPU utilization 


 kubectl top pod -n name
 
=============================================

16) Create a deployment as follows
name: nginx-random
exposed via service: nginx-random
Ensure that the respective pod and service are accessible via their DNS records
The containers with in any pod should use Nginx image
 && write the output of DNS records to the files
 
 
 ANS:
 ------
 
 
 Kubectl create deployment nginx-random --image=nginx
 
 kubectl expose deployment\nginx-random --name=nginx-random
 
 
 ===========================================================
 
17) Create a snaoshot of the etcd instance running at https://127.0.0.1:2379  saving the snapshot at /opt/snapshot.db

etcd instance runnning etcd version 3.3.10
CA cert: /opt/kum302/ca.crt
client cert: /opt/kum302/etcd-client.crt
client key: /opt/kum302/etcd-client.key



=======================================================

18) set the node named node-0 as unavailable and reschedule all pods running on its

kubectl drain node-0 --ignore-daemonsets

kubectl cordon my-node

=========================================================

19) Kubernetes worker node is in not-ready state --> bring that to ready state


==========================================================

20) configure a static pod on node01


==========================================================

21)  configure master and node using Kubeadm

swapoff -a

install kubeadm kubectl and kubelet
then --> kube init -- select the configuration file --> join node  --> join network

=========================================================

22) Troubleshooting

Partially working node  ---> kubectl get nodes -- not working


--------------------------------------------------------
23) create a pv named: app-config of capa 2Gi and access mode readwritemany  , type volume hostpaths its location: /srv/app-config

============================================================
 
====================================================== 


apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-config
spec:
  capacity:
    storage: 2Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/tmp"
	
	
	===================================
	
24) create single instance of pod on every node (i.e. daemonset). do not alter taints on node.

============================================
